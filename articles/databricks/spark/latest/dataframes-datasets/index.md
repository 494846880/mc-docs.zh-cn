---
ms.topic: conceptual
ms.service: azure-databricks
ms.reviewer: mamccrea
ms.custom: databricksmigration
ms.author: saperla
author: mssaperla
ms.date: 08/20/2020
title: 数据帧和数据集 - Azure Databricks
description: 了解如何在 Azure Databricks 中使用 Apache Spark 数据帧和数据集。
ms.openlocfilehash: 5aecbce6c72c1c9ffb5e18d7774a04e87852ec9f
ms.sourcegitcommit: 537d52cb783892b14eb9b33cf29874ffedebbfe3
ms.translationtype: HT
ms.contentlocale: zh-CN
ms.lasthandoff: 10/23/2020
ms.locfileid: "92473029"
---
# <a name="dataframes-and-datasets"></a>数据帧和数据集

此部分通过 Azure Databricks 笔记本介绍了 Apache Spark 数据帧和数据集。

* [数据帧简介 - Python](introduction-to-dataframes-python.md)
  * [创建数据帧](introduction-to-dataframes-python.md#create-dataframes)
  * [使用数据帧](introduction-to-dataframes-python.md#work-with-dataframes)
  * [数据帧常见问题解答](introduction-to-dataframes-python.md#dataframe-faqs)
* [数据帧简介 - Scala](introduction-to-dataframes-scala.md)
  * [创建数据帧](introduction-to-dataframes-scala.md#create-dataframes)
  * [使用数据帧](introduction-to-dataframes-scala.md#work-with-dataframes)
  * [常见问题解答 (FAQ)](introduction-to-dataframes-scala.md#frequently-asked-questions-faq)
* [数据集简介](introduction-to-datasets.md)
  * [创建数据集](introduction-to-datasets.md#create-a-dataset)
  * [使用数据集](introduction-to-datasets.md#work-with-datasets)
  * [将数据集转换为数据帧](introduction-to-datasets.md#convert-a-dataset-to-a-dataframe)
* [复杂数据和嵌套数据](complex-nested-data.md)
  * [复杂嵌套数据笔记本](complex-nested-data.md#complex-nested-data-notebook)
* [聚合器](aggregators.md)
  * [数据集聚合器笔记本](aggregators.md#dataset-aggregator-notebook)
* [日期和时间戳](dates-timestamps.md)
  * [日期和日历](dates-timestamps.md#dates-and-calendars)
  * [时间戳和时区](dates-timestamps.md#timestamps-and-time-zones)
  * [构造时间和时间戳](dates-timestamps.md#construct-dates-and-timestamps)
  * [收集时间和时间戳](dates-timestamps.md#collect-dates-and-timestamps)

对于数据帧和数据集的参考信息，Azure Databricks 建议使用以下 Apache Spark API 参考：

* [Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html)
* [Scala API](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)
* [Java API](https://spark.apache.org/docs/latest/api/java/index.html)